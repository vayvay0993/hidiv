{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import src.cathay_db as db\n",
    "import src.utils as ut\n",
    "import src.financial_statement as fs\n",
    "\n",
    "reload(ut)\n",
    "reload(fs)\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "import xgboost as xgb\n",
    "sys.path.append(\"./src\")\n",
    "\n",
    "from toolbox import print_progress_bar\n",
    "\n",
    "# set max display rows\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "# set max display columns\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "# Set the float format to display without scientific notation\n",
    "pd.options.display.float_format = '{:.4f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['start_index', 'rebalance_date_lst', 'factor_columns', 'target_cols', 'model_save_path', 'param_search', 'fit_params'])\n",
      "['asset_qoq', 'asset_yoy', 'ni_qoq', 'ni_yoy', 'roe', 'roe_yoy', 'roe_4q_sum', 'roe_4q_sum_yoy', 'tobins_q', 'ocf / asset', '20_d_return', '40_d_return', '60_d_return', 'dividend_1Y_sum_yield', 'dividend_2Y_sum_yield', 'dividend_3Y_sum_yield', 'last_dividend_yield']\n"
     ]
    }
   ],
   "source": [
    "# select model \n",
    "model_folder_name = '20230719_083148'\n",
    "# model_folder_name = '20230712_173216'\n",
    "# model_folder_name = '20230711_171428'\n",
    "# model_folder_name = '20230711_110457'\n",
    "# model_folder_name = '20230707_140403'\n",
    "# model_folder_name = '20230706_141045'\n",
    "\n",
    "# load setting from data/model/model_folder_name/setting.json\n",
    "setting = ut.load_json(f'./data/model/{model_folder_name}/setting.json')\n",
    "\n",
    "print(setting.keys())\n",
    "print(setting['factor_columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list for model json file in model/model_folder_name\n",
    "model_list = []\n",
    "for model in os.listdir(f'./models/{model_folder_name}'):\n",
    "    if model.endswith('.json'):\n",
    "        # append the model path to model_list\n",
    "        model_list.append(f'./models/{model_folder_name}/{model}')\n",
    "# sort model_list\n",
    "model_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df_factor from data/model/model_folder_name/df_factor_all.feather\n",
    "df_factor_all = pd.read_feather(f'./data/model/{model_folder_name}/df_factor_all.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df_factor from data/model/model_folder_name/df_factor_all.feather\n",
    "df_factor_all = pd.read_feather(f'./data/model/{model_folder_name}/df_factor_all.feather')\n",
    "\n",
    "# create a dataframe for model mapping\n",
    "df_model_mapping = pd.DataFrame()\n",
    "df_model_mapping['rebalance_date'] = setting['rebalance_date_lst'][setting['start_index'] + 0:]\n",
    "df_model_mapping['model_path'] = model_list\n",
    "# convert rebalance_date to datetime\n",
    "df_model_mapping['rebalance_date'] = pd.to_datetime(df_model_mapping['rebalance_date'])\n",
    "\n",
    "# merge the df_model_mapping and df_factor_all\n",
    "df_factor_all = pd.merge(df_factor_all, df_model_mapping, left_on='date', right_on='rebalance_date', how='left')\n",
    "df_factor_all = df_factor_all.sort_values(['date','ticker']).reset_index(drop=True)\n",
    "df_factor_all['model_path'].fillna(method='ffill', inplace=True)\n",
    "df_factor_all['rebalance_date'].fillna(method='ffill', inplace=True)\n",
    "df_factor_all.dropna(subset=['model_path'], inplace=True)\n",
    "\n",
    "df_factor_all = df_factor_all.reset_index(drop=True)\n",
    "df_factor_all.to_feather(f'./data/model/{model_folder_name}/df_factor_all_select.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_columns = setting['factor_columns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = df_factor_all['model_path'].unique()\n",
    "reb_lst = df_factor_all['rebalance_date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_all = pd.DataFrame([])\n",
    "df_feature_importance_all = pd.DataFrame([])\n",
    "for i, model_path in enumerate(model_list):\n",
    "    model = xgb.Booster()\n",
    "    model.load_model(model_path)\n",
    "    try:\n",
    "        df_feature_importance = pd.DataFrame(model.get_score(importance_type='gain').items(), columns=['feature','importance'])\n",
    "        df_feature_importance.sort_values('importance', ascending=False, inplace=True)\n",
    "        df_feature_importance['date'] = reb_lst[i]\n",
    "        df_feature_importance_all = pd.concat([df_feature_importance_all, df_feature_importance], axis=0)\n",
    "\n",
    "        df_predict = df_factor_all[df_factor_all['rebalance_date'] == reb_lst[i]].reset_index(drop=True).copy()\n",
    "        df_predict['predict'] = model.predict(xgb.DMatrix(df_predict[factor_columns]))\n",
    "        df_predict_all = pd.concat([df_predict_all, df_predict], axis=0)\n",
    "    except:\n",
    "        print(reb_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>IR</th>\n",
       "      <th>mean_rank</th>\n",
       "      <th>IR_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roe</td>\n",
       "      <td>72.0000</td>\n",
       "      <td>129.2931</td>\n",
       "      <td>29.0219</td>\n",
       "      <td>81.6439</td>\n",
       "      <td>105.7983</td>\n",
       "      <td>122.7689</td>\n",
       "      <td>152.2346</td>\n",
       "      <td>185.4275</td>\n",
       "      <td>4.2302</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>7.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ni_yoy</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>92.5848</td>\n",
       "      <td>36.9257</td>\n",
       "      <td>25.8884</td>\n",
       "      <td>53.1995</td>\n",
       "      <td>102.8310</td>\n",
       "      <td>125.2072</td>\n",
       "      <td>148.9049</td>\n",
       "      <td>2.7848</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>14.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tobins_q</td>\n",
       "      <td>72.0000</td>\n",
       "      <td>90.7967</td>\n",
       "      <td>34.0442</td>\n",
       "      <td>33.1311</td>\n",
       "      <td>62.4043</td>\n",
       "      <td>88.3939</td>\n",
       "      <td>124.9307</td>\n",
       "      <td>146.3965</td>\n",
       "      <td>2.5964</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>15.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>last_dividend_yield</td>\n",
       "      <td>72.0000</td>\n",
       "      <td>87.8232</td>\n",
       "      <td>14.4876</td>\n",
       "      <td>45.8703</td>\n",
       "      <td>84.9974</td>\n",
       "      <td>92.5041</td>\n",
       "      <td>96.4299</td>\n",
       "      <td>108.8885</td>\n",
       "      <td>6.3851</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roe_4q_sum</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>79.3873</td>\n",
       "      <td>15.1367</td>\n",
       "      <td>39.2368</td>\n",
       "      <td>71.8969</td>\n",
       "      <td>84.2529</td>\n",
       "      <td>90.0139</td>\n",
       "      <td>105.7890</td>\n",
       "      <td>5.5661</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>2.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60_d_return</td>\n",
       "      <td>72.0000</td>\n",
       "      <td>78.2667</td>\n",
       "      <td>30.5038</td>\n",
       "      <td>30.7428</td>\n",
       "      <td>53.3122</td>\n",
       "      <td>74.8509</td>\n",
       "      <td>95.0571</td>\n",
       "      <td>152.1602</td>\n",
       "      <td>2.4538</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>16.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dividend_1Y_sum_yield</td>\n",
       "      <td>71.0000</td>\n",
       "      <td>75.8732</td>\n",
       "      <td>42.3117</td>\n",
       "      <td>13.9206</td>\n",
       "      <td>37.5700</td>\n",
       "      <td>77.2768</td>\n",
       "      <td>109.6611</td>\n",
       "      <td>170.4933</td>\n",
       "      <td>1.8264</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>17.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>roe_yoy</td>\n",
       "      <td>70.0000</td>\n",
       "      <td>74.6743</td>\n",
       "      <td>27.5687</td>\n",
       "      <td>11.6989</td>\n",
       "      <td>46.5482</td>\n",
       "      <td>82.6170</td>\n",
       "      <td>96.8557</td>\n",
       "      <td>129.3008</td>\n",
       "      <td>2.9968</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>13.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40_d_return</td>\n",
       "      <td>72.0000</td>\n",
       "      <td>68.4778</td>\n",
       "      <td>13.4379</td>\n",
       "      <td>39.0896</td>\n",
       "      <td>57.5964</td>\n",
       "      <td>69.2749</td>\n",
       "      <td>78.9969</td>\n",
       "      <td>94.6772</td>\n",
       "      <td>5.1552</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>5.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20_d_return</td>\n",
       "      <td>72.0000</td>\n",
       "      <td>67.6521</td>\n",
       "      <td>15.6227</td>\n",
       "      <td>40.4147</td>\n",
       "      <td>54.6327</td>\n",
       "      <td>65.8863</td>\n",
       "      <td>75.0778</td>\n",
       "      <td>103.6699</td>\n",
       "      <td>4.2173</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>8.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ocf / asset</td>\n",
       "      <td>72.0000</td>\n",
       "      <td>66.8731</td>\n",
       "      <td>21.0049</td>\n",
       "      <td>31.1727</td>\n",
       "      <td>50.8733</td>\n",
       "      <td>63.0800</td>\n",
       "      <td>84.3585</td>\n",
       "      <td>117.0711</td>\n",
       "      <td>3.0031</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>12.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ni_qoq</td>\n",
       "      <td>72.0000</td>\n",
       "      <td>65.4506</td>\n",
       "      <td>20.7947</td>\n",
       "      <td>27.1623</td>\n",
       "      <td>46.9072</td>\n",
       "      <td>70.5129</td>\n",
       "      <td>82.8993</td>\n",
       "      <td>101.2401</td>\n",
       "      <td>3.3909</td>\n",
       "      <td>12.0000</td>\n",
       "      <td>11.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dividend_2Y_sum_yield</td>\n",
       "      <td>72.0000</td>\n",
       "      <td>61.2334</td>\n",
       "      <td>16.1358</td>\n",
       "      <td>31.0588</td>\n",
       "      <td>47.9461</td>\n",
       "      <td>62.8129</td>\n",
       "      <td>72.6239</td>\n",
       "      <td>103.7743</td>\n",
       "      <td>3.8928</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>9.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>asset_yoy</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>60.5440</td>\n",
       "      <td>11.3669</td>\n",
       "      <td>23.5536</td>\n",
       "      <td>53.8605</td>\n",
       "      <td>62.7778</td>\n",
       "      <td>70.2019</td>\n",
       "      <td>77.0051</td>\n",
       "      <td>5.5229</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>3.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>roe_4q_sum_yoy</td>\n",
       "      <td>72.0000</td>\n",
       "      <td>58.6166</td>\n",
       "      <td>11.4924</td>\n",
       "      <td>31.2042</td>\n",
       "      <td>52.3814</td>\n",
       "      <td>60.6388</td>\n",
       "      <td>65.5881</td>\n",
       "      <td>83.3004</td>\n",
       "      <td>5.2764</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>4.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>asset_qoq</td>\n",
       "      <td>72.0000</td>\n",
       "      <td>53.8678</td>\n",
       "      <td>14.7574</td>\n",
       "      <td>30.9693</td>\n",
       "      <td>41.5607</td>\n",
       "      <td>53.6570</td>\n",
       "      <td>64.6569</td>\n",
       "      <td>89.9701</td>\n",
       "      <td>3.6359</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>10.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dividend_3Y_sum_yield</td>\n",
       "      <td>72.0000</td>\n",
       "      <td>51.7759</td>\n",
       "      <td>11.5029</td>\n",
       "      <td>33.3626</td>\n",
       "      <td>43.0009</td>\n",
       "      <td>49.1705</td>\n",
       "      <td>58.7422</td>\n",
       "      <td>86.6886</td>\n",
       "      <td>4.2746</td>\n",
       "      <td>17.0000</td>\n",
       "      <td>6.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature   count     mean     std     min      25%      50%  \\\n",
       "0                     roe 72.0000 129.2931 29.0219 81.6439 105.7983 122.7689   \n",
       "1                  ni_yoy 69.0000  92.5848 36.9257 25.8884  53.1995 102.8310   \n",
       "2                tobins_q 72.0000  90.7967 34.0442 33.1311  62.4043  88.3939   \n",
       "3     last_dividend_yield 72.0000  87.8232 14.4876 45.8703  84.9974  92.5041   \n",
       "4              roe_4q_sum 69.0000  79.3873 15.1367 39.2368  71.8969  84.2529   \n",
       "5             60_d_return 72.0000  78.2667 30.5038 30.7428  53.3122  74.8509   \n",
       "6   dividend_1Y_sum_yield 71.0000  75.8732 42.3117 13.9206  37.5700  77.2768   \n",
       "7                 roe_yoy 70.0000  74.6743 27.5687 11.6989  46.5482  82.6170   \n",
       "8             40_d_return 72.0000  68.4778 13.4379 39.0896  57.5964  69.2749   \n",
       "9             20_d_return 72.0000  67.6521 15.6227 40.4147  54.6327  65.8863   \n",
       "10            ocf / asset 72.0000  66.8731 21.0049 31.1727  50.8733  63.0800   \n",
       "11                 ni_qoq 72.0000  65.4506 20.7947 27.1623  46.9072  70.5129   \n",
       "12  dividend_2Y_sum_yield 72.0000  61.2334 16.1358 31.0588  47.9461  62.8129   \n",
       "13              asset_yoy 69.0000  60.5440 11.3669 23.5536  53.8605  62.7778   \n",
       "14         roe_4q_sum_yoy 72.0000  58.6166 11.4924 31.2042  52.3814  60.6388   \n",
       "15              asset_qoq 72.0000  53.8678 14.7574 30.9693  41.5607  53.6570   \n",
       "16  dividend_3Y_sum_yield 72.0000  51.7759 11.5029 33.3626  43.0009  49.1705   \n",
       "\n",
       "        75%      max     IR  mean_rank  IR_rank  \n",
       "0  152.2346 185.4275 4.2302     1.0000   7.0000  \n",
       "1  125.2072 148.9049 2.7848     2.0000  14.0000  \n",
       "2  124.9307 146.3965 2.5964     3.0000  15.0000  \n",
       "3   96.4299 108.8885 6.3851     4.0000   1.0000  \n",
       "4   90.0139 105.7890 5.5661     5.0000   2.0000  \n",
       "5   95.0571 152.1602 2.4538     6.0000  16.0000  \n",
       "6  109.6611 170.4933 1.8264     7.0000  17.0000  \n",
       "7   96.8557 129.3008 2.9968     8.0000  13.0000  \n",
       "8   78.9969  94.6772 5.1552     9.0000   5.0000  \n",
       "9   75.0778 103.6699 4.2173    10.0000   8.0000  \n",
       "10  84.3585 117.0711 3.0031    11.0000  12.0000  \n",
       "11  82.8993 101.2401 3.3909    12.0000  11.0000  \n",
       "12  72.6239 103.7743 3.8928    13.0000   9.0000  \n",
       "13  70.2019  77.0051 5.5229    14.0000   3.0000  \n",
       "14  65.5881  83.3004 5.2764    15.0000   4.0000  \n",
       "15  64.6569  89.9701 3.6359    16.0000  10.0000  \n",
       "16  58.7422  86.6886 4.2746    17.0000   6.0000  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature_importance_des = df_feature_importance_all.groupby('feature')['importance'].describe().sort_values('mean', ascending=False).reset_index()\n",
    "df_feature_importance_des['IR'] = df_feature_importance_des['50%'] / df_feature_importance_des['std']\n",
    "# rank IR\n",
    "df_feature_importance_des['mean_rank'] = df_feature_importance_des['mean'].rank(ascending=False)\n",
    "df_feature_importance_des['IR_rank'] = df_feature_importance_des['IR'].rank(ascending=False)\n",
    "\n",
    "df_feature_importance_des.sort_values('mean', ascending=False, inplace=True)\n",
    "df_feature_importance_des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "df_predict_describe_all = pd.DataFrame([])\n",
    "\n",
    "for i in range(len(reb_lst)):\n",
    "    df_predict = df_predict_all[df_predict_all['rebalance_date'] == reb_lst[i]].reset_index(drop=True).copy()\n",
    "    # calc predict qcut\n",
    "    df_predict['predict_qcut'] = pd.qcut(df_predict['predict'], 10, labels=False, duplicates='drop')\n",
    "    df_predict_describe = df_predict.groupby('predict_qcut')['y'].describe().reset_index()\n",
    "    df_predict_describe['date'] = reb_lst[i]\n",
    "    df_predict_describe_all = pd.concat([df_predict_describe_all, df_predict_describe], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predict_qcut</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>72.0000</td>\n",
       "      <td>0.0633</td>\n",
       "      <td>0.1049</td>\n",
       "      <td>-0.2643</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0647</td>\n",
       "      <td>0.0987</td>\n",
       "      <td>0.4599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>72.0000</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>-0.2658</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0551</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>0.4997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>72.0000</td>\n",
       "      <td>0.0486</td>\n",
       "      <td>0.1034</td>\n",
       "      <td>-0.2854</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.0910</td>\n",
       "      <td>0.4401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>72.0000</td>\n",
       "      <td>0.0445</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>-0.2921</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0446</td>\n",
       "      <td>0.0805</td>\n",
       "      <td>0.4162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>72.0000</td>\n",
       "      <td>0.0434</td>\n",
       "      <td>0.1028</td>\n",
       "      <td>-0.3050</td>\n",
       "      <td>-0.0089</td>\n",
       "      <td>0.0439</td>\n",
       "      <td>0.0897</td>\n",
       "      <td>0.4422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72.0000</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>0.1012</td>\n",
       "      <td>-0.3230</td>\n",
       "      <td>-0.0071</td>\n",
       "      <td>0.0415</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.4596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72.0000</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>-0.3211</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.0809</td>\n",
       "      <td>0.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72.0000</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>-0.3471</td>\n",
       "      <td>-0.0073</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.0741</td>\n",
       "      <td>0.4785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72.0000</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.1212</td>\n",
       "      <td>-0.3558</td>\n",
       "      <td>-0.0146</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>0.0819</td>\n",
       "      <td>0.5149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72.0000</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>-0.3383</td>\n",
       "      <td>-0.0251</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.0725</td>\n",
       "      <td>0.4776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count   mean    std     min     25%    50%    75%    max\n",
       "predict_qcut                                                           \n",
       "9            72.0000 0.0633 0.1049 -0.2643  0.0203 0.0647 0.0987 0.4599\n",
       "8            72.0000 0.0533 0.1040 -0.2658  0.0074 0.0551 0.0849 0.4997\n",
       "7            72.0000 0.0486 0.1034 -0.2854  0.0026 0.0462 0.0910 0.4401\n",
       "6            72.0000 0.0445 0.1010 -0.2921  0.0026 0.0446 0.0805 0.4162\n",
       "5            72.0000 0.0434 0.1028 -0.3050 -0.0089 0.0439 0.0897 0.4422\n",
       "4            72.0000 0.0389 0.1012 -0.3230 -0.0071 0.0415 0.0717 0.4596\n",
       "3            72.0000 0.0365 0.1057 -0.3211 -0.0007 0.0383 0.0809 0.4500\n",
       "2            72.0000 0.0328 0.1110 -0.3471 -0.0073 0.0332 0.0741 0.4785\n",
       "1            72.0000 0.0323 0.1212 -0.3558 -0.0146 0.0276 0.0819 0.5149\n",
       "0            72.0000 0.0221 0.1177 -0.3383 -0.0251 0.0212 0.0725 0.4776"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predict_describe_all.groupby('predict_qcut')['mean'].describe().sort_values('mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_all = df_predict_all[df_predict_all['date'].isin(reb_lst)]\n",
    "# df_predict_all['predict_group'] = df_predict_all.groupby('rebalance_date',group_keys=False)['predict'].apply(lambda x :pd.qcut(x, 5, labels=False))\n",
    "df_predict_all['predict_rank'] = df_predict_all.groupby('rebalance_date',group_keys=False)['predict'].rank(ascending=False)\n",
    "df_select = df_predict_all[df_predict_all['predict_rank']<=50]\n",
    "df_select_return = df_select.groupby('date')['y'].mean()\n",
    "\n",
    "\n",
    "def calculate_cagr(df):\n",
    "    # Add 1 to all the quarterly returns\n",
    "    df_plus_one = df + 1\n",
    "    # Calculate the cumulative product of the returns\n",
    "    cum_product = df_plus_one.cumprod()\n",
    "    # Get the total number of years\n",
    "    num_years = len(df) / 4\n",
    "    # Calculate CAGR\n",
    "    cagr = (cum_product.iloc[-1])**(1/num_years) - 1\n",
    "    return cagr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR: 0.91\n",
      "CAGR: 22.95%\n",
      "Std: 25.31%\n"
     ]
    }
   ],
   "source": [
    "print(f\"SR: {((df_select_return + 1).prod()**(4/len(df_select_return)) -1 ) / (df_select_return.std() * np.sqrt(4)):.2f}\")\n",
    "# Assuming your DataFrame is named df_returns\n",
    "cagr = calculate_cagr(df_select_return)\n",
    "print(f\"CAGR: {cagr:.2%}\")\n",
    "print(f\"Std: {(df_select_return.std() * np.sqrt(4)):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow code is not used because the top stocks is select by last dividend yield and predict_rank, \n",
    "# the result will fulfill the requirement (dividend yield >= 5% and predict_rank <= certain rank)\n",
    "\n",
    "# dividend_min = 0.05\n",
    "# df_dividend_filter = df_predict_all[df_predict_all['last_dividend_yield'] >= dividend_min].copy()\n",
    "# df_predict_filter = df_predict_all[df_predict_all['predict_rank'] <= 300]\n",
    "# df_dividend_select = pd.concat([df_dividend_filter, df_predict_filter]).drop_duplicates().reset_index(drop=True)\n",
    "# df_dividend_select.sort_values(['date','ticker'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consider Market Cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_market_cap_melt.to_feather('./data/df_market_cap_melt.feather')\n",
    "df_market_cap_melt = pd.read_feather('./data/df_market_cap_melt.feather')\n",
    "df_predict_merge_cap = df_predict_all.merge(df_market_cap_melt, on=['date','ticker'], how='left').copy()\n",
    "df_predict_merge_cap['market_cap_rank'] = df_predict_merge_cap.groupby('date',group_keys=False)['market_cap'].rank(ascending=False)\n",
    "df_predict_merge_cap = df_predict_merge_cap[df_predict_merge_cap['market_cap_rank'] <= 300].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consider Liquidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_price_liquidity_flag.to_feather('./data/df_price_liquidity_flag.feather')\n",
    "df_price_liquidity_flag = pd.read_feather('./data/df_price_liquidity_flag.feather')\n",
    "df_price_liquidity_flag = df_price_liquidity_flag[['日期','股票代號','flag_all']]\n",
    "df_price_liquidity_flag.columns = ['date','ticker','flag_all']\n",
    "\n",
    "\n",
    "df_predict_merge_cap = pd.merge(df_predict_merge_cap, df_price_liquidity_flag, on=['date','ticker'], how='left')\n",
    "df_select_pool = df_predict_merge_cap[df_predict_merge_cap['flag_all'] != 1].reset_index(drop=True).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不做選股Buffer | 依次排序 1.股利率 2.模型預測\n",
      "SR: 0.62\n",
      "CAGR: 13.34%\n",
      "Std: 21.56%\n"
     ]
    }
   ],
   "source": [
    "def top_n(df, n=5, columns=['last_dividend_yield', 'predict_rank'], ascending=[False, True]):\n",
    "    return df.sort_values(by=columns, ascending=ascending).head(n)\n",
    "\n",
    "df_select_pool = df_select_pool.reset_index(drop=True)\n",
    "df_select_pool_top_50 = df_select_pool.groupby('date').apply(top_n, n=50, columns=['last_dividend_yield', 'predict_rank'], ascending=[False, True])\n",
    "\n",
    "df_select_pool_top_50.reset_index(drop=True, inplace=True)\n",
    "df_select_pool_top_50_return = df_select_pool_top_50.groupby('date')['y'].mean()\n",
    "print(\"不做選股Buffer | 依次排序 1.股利率 2.模型預測\")\n",
    "print(f\"SR: {((df_select_pool_top_50_return + 1).prod() ** (4/len(df_select_pool_top_50_return)) -1 ) / (df_select_pool_top_50_return.std() * np.sqrt(4)):.2f}\")\n",
    "# Assuming your DataFrame is named df_returns\n",
    "cagr = calculate_cagr(df_select_pool_top_50_return)\n",
    "print(f\"CAGR: {cagr:.2%}\")\n",
    "print(f\"Std: {(df_select_pool_top_50_return.std() * np.sqrt(4)):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不做選股Buffer | 只排序 1.模型預測\n",
      "SR: 0.76\n",
      "CAGR: 16.49%\n",
      "Std: 21.81%\n"
     ]
    }
   ],
   "source": [
    "df_select_pool = df_select_pool.reset_index(drop=True)\n",
    "df_select_pool_top_50 = df_select_pool.groupby('date').apply(top_n, n=50, columns=['predict_rank'], ascending=[True])\n",
    "\n",
    "df_select_pool_top_50.reset_index(drop=True, inplace=True)\n",
    "df_select_pool_top_50_return = df_select_pool_top_50.groupby('date')['y'].mean()\n",
    "print(\"不做選股Buffer | 只排序 1.模型預測\")\n",
    "print(f\"SR: {((df_select_pool_top_50_return + 1).prod() ** (4/len(df_select_pool_top_50_return)) -1 ) / (df_select_pool_top_50_return.std() * np.sqrt(4)):.2f}\")\n",
    "# Assuming your DataFrame is named df_returns\n",
    "cagr = calculate_cagr(df_select_pool_top_50_return)\n",
    "print(f\"CAGR: {cagr:.2%}\")\n",
    "print(f\"Std: {(df_select_pool_top_50_return.std() * np.sqrt(4)):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool 不做選股Buffer | 只排序 1.市值\n",
      "SR: 0.44\n",
      "CAGR: 8.18%\n",
      "Std: 18.51%\n"
     ]
    }
   ],
   "source": [
    "df_select_pool_benchmark = df_select_pool.groupby('date').apply(top_n, n=50, columns=['market_cap_rank'], ascending=[True])\n",
    "\n",
    "df_select_pool_benchmark.reset_index(drop=True, inplace=True)\n",
    "df_select_pool_benchmark_return = df_select_pool_benchmark.groupby('date')['y'].mean()\n",
    "print(\"Pool 不做選股Buffer | 只排序 1.市值\")\n",
    "print(f\"SR: {((df_select_pool_benchmark_return + 1).prod() ** (4/len(df_select_pool_benchmark_return)) -1 ) / (df_select_pool_benchmark_return.std() * np.sqrt(4)):.2f}\")\n",
    "# Assuming your DataFrame is named df_returns\n",
    "cagr = calculate_cagr(df_select_pool_benchmark_return)\n",
    "print(f\"CAGR: {cagr:.2%}\")\n",
    "print(f\"Std: {(df_select_pool_benchmark_return.std() * np.sqrt(4)):.2%}\")\n",
    "\n",
    "df_select_pool_benchmark.to_feather(f'./data/model/{model_folder_name}/df_select_pool_benchmark.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select_pool.to_feather(f'./data/model/{model_folder_name}/df_select_pool.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-filtering the dividend yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_market_cap_melt.to_feather('./data/df_market_cap_melt.feather')\n",
    "df_market_cap_melt = pd.read_feather('./data/df_market_cap_melt.feather')\n",
    "df_predict_merge_cap = df_predict_all.merge(df_market_cap_melt, on=['date','ticker'], how='left').copy()\n",
    "df_predict_merge_cap['market_cap_rank'] = df_predict_merge_cap.groupby('date',group_keys=False)['market_cap'].rank(ascending=False)\n",
    "df_predict_merge_cap = df_predict_merge_cap[df_predict_merge_cap['market_cap_rank'] <= 300].copy()\n",
    "\n",
    "df_filter_dividends_yield = df_predict_merge_cap[df_predict_merge_cap['dividend_1Y_sum_yield'] > 0.04].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider Liquidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    72.0000\n",
      "mean    139.5417\n",
      "std      37.4034\n",
      "min      61.0000\n",
      "25%     111.2500\n",
      "50%     145.5000\n",
      "75%     162.2500\n",
      "max     239.0000\n",
      "Name: y, dtype: float64\n",
      "不做選股Buffer | 只排序 1.模型預測\n",
      "SR: 0.87\n",
      "CAGR: 16.73%\n",
      "Std: 19.33%\n"
     ]
    }
   ],
   "source": [
    "# df_price_liquidity_flag.to_feather('./data/df_price_liquidity_flag.feather')\n",
    "df_price_liquidity_flag = pd.read_feather('./data/df_price_liquidity_flag.feather')\n",
    "df_price_liquidity_flag = df_price_liquidity_flag[['日期','股票代號','flag_all']]\n",
    "df_price_liquidity_flag.columns = ['date','ticker','flag_all']\n",
    "\n",
    "df_filter_dividends_yield = pd.merge(df_filter_dividends_yield, df_price_liquidity_flag, on=['date','ticker'], how='left')\n",
    "df_select_filter_dividends_yield = df_filter_dividends_yield[df_filter_dividends_yield['flag_all'] != 1].reset_index(drop=True).copy()\n",
    "print(df_select_filter_dividends_yield.groupby('date')['y'].count().describe())\n",
    "\n",
    "df_select_filter_dividends_yield = df_select_filter_dividends_yield.reset_index(drop=True)\n",
    "df_select_filter_dividends_yield_top_50 = df_select_filter_dividends_yield.groupby('date').apply(top_n, n=50, columns=['predict_rank'], ascending=[True])\n",
    "\n",
    "df_select_filter_dividends_yield_top_50.reset_index(drop=True, inplace=True)\n",
    "df_select_filter_dividends_yield_top_50_return = df_select_filter_dividends_yield_top_50.groupby('date')['y'].mean()\n",
    "print(\"不做選股Buffer | 只排序 1.模型預測\")\n",
    "print(f\"SR: {((df_select_filter_dividends_yield_top_50_return + 1).prod() ** (4/len(df_select_filter_dividends_yield_top_50_return)) -1 ) / (df_select_filter_dividends_yield_top_50_return.std() * np.sqrt(4)):.2f}\")\n",
    "# Assuming your DataFrame is named df_returns\n",
    "cagr = calculate_cagr(df_select_filter_dividends_yield_top_50_return)\n",
    "print(f\"CAGR: {cagr:.2%}\")\n",
    "print(f\"Std: {(df_select_filter_dividends_yield_top_50_return.std() * np.sqrt(4)):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select_filter_dividends_yield_top_50.to_feather(f'./data/model/{model_folder_name}/df_select_filter_dividends_yield_top_50.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool 不做選股Buffer | 只排序 1.市值\n",
      "SR: 0.50\n",
      "CAGR: 9.32%\n",
      "Std: 18.53%\n"
     ]
    }
   ],
   "source": [
    "df_select_filter_dividends_yield_benchmark = df_select_filter_dividends_yield.groupby('date').apply(top_n, n=50, columns=['market_cap_rank'], ascending=[True])\n",
    "\n",
    "df_select_filter_dividends_yield_benchmark.reset_index(drop=True, inplace=True)\n",
    "df_select_filter_dividends_yield_benchmark_return = df_select_filter_dividends_yield_benchmark.groupby('date')['y'].mean()\n",
    "print(\"Pool 不做選股Buffer | 只排序 1.市值\")\n",
    "print(f\"SR: {((df_select_filter_dividends_yield_benchmark_return + 1).prod() ** (4/len(df_select_filter_dividends_yield_benchmark_return)) -1 ) / (df_select_filter_dividends_yield_benchmark_return.std() * np.sqrt(4)):.2f}\")\n",
    "# Assuming your DataFrame is named df_returns\n",
    "cagr = calculate_cagr(df_select_filter_dividends_yield_benchmark_return)\n",
    "print(f\"CAGR: {cagr:.2%}\")\n",
    "print(f\"Std: {(df_select_filter_dividends_yield_benchmark_return.std() * np.sqrt(4)):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select_filter_dividends_yield_benchmark.to_feather(f'./data/model/{model_folder_name}/df_select_filter_dividends_yield_benchmark.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restrict Turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自 2005-06-15 開始使用 restrict 建構投組\n",
      "df_restrict_result['date'].max(): 2005-03-15 00:00:00\n",
      "./data/model/20230719_083148/df_restrict_result_1_50_60\n",
      "SR: 0.70\n",
      "CAGR: 13.68%\n",
      "Std: 19.52%\n"
     ]
    }
   ],
   "source": [
    "apply_restrict_index = 1\n",
    "base_portfolio_number = 50\n",
    "restrict_turnover = 0.2\n",
    "\n",
    "remain_number =  int(np.ceil(base_portfolio_number * (1 - restrict_turnover)))\n",
    "\n",
    "# round up the restrict_portfolio_number\n",
    "restrict_portfolio_number =  (base_portfolio_number * 2 - remain_number)\n",
    "\n",
    "df_select_pool = df_select_filter_dividends_yield.reset_index(drop=True)\n",
    "# get the base portfolio by base_portfolio_number \n",
    "df_select_pool_base = df_select_pool.groupby('date').apply(top_n, n = base_portfolio_number, columns=['predict_rank'], ascending=[True]).reset_index(drop=True)\n",
    "# get the restrict portfolio by restrict_portfolio_number\n",
    "df_select_pool_restrict = df_select_pool\n",
    "df_select_pool_restrict.reset_index(drop=True, inplace=True)\n",
    "\n",
    "apply_restrict_date = np.sort(reb_lst)[apply_restrict_index]\n",
    "# string format\n",
    "apply_restrict_date = pd.to_datetime(apply_restrict_date).strftime('%Y-%m-%d')\n",
    "print(f'自 {apply_restrict_date} 開始使用 restrict 建構投組')\n",
    "\n",
    "df_restrict_result = pd.DataFrame([])\n",
    "# concat the data that date is smaller than apply_restrict_date to df_restrict_result\n",
    "df_restrict_result = pd.concat([df_restrict_result, df_select_pool_base[df_select_pool_base['date'] < apply_restrict_date]], axis=0)\n",
    "# print the max date in df_restrict_result\n",
    "print(f\"df_restrict_result['date'].max(): {df_restrict_result['date'].max()}\")\n",
    "\n",
    "\n",
    "for i in range(apply_restrict_index, len(reb_lst)):\n",
    "# i = 1\n",
    "\n",
    "    # get the slice data by reb_lst[i] in df_select_pool_restrict\n",
    "    df_select_pool_restrict_temp = df_select_pool_restrict[df_select_pool_restrict['rebalance_date'] == reb_lst[i]].reset_index(drop=True)\n",
    "\n",
    "    # get the slice data by reb_lst[i-1] in df_restrict_result, [i-1] is because we need to get the last portfolio\n",
    "    df_restrict_result_temp = df_restrict_result[df_restrict_result['rebalance_date'] == reb_lst[i - 1]].reset_index(drop=True)\n",
    "\n",
    "    # prepare the dataframe for finding the joint ticker\n",
    "    df_new_temp = df_select_pool_restrict_temp[['date','ticker','predict', 'dividend_1Y_sum_yield']].copy()\n",
    "    df_new_temp['restrict'] = 1\n",
    "    df_new_temp.sort_values(['predict','dividend_1Y_sum_yield'], ascending=False, inplace=True)\n",
    "    df_new_temp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    last_portfolio_ticker_list = df_restrict_result_temp['ticker'].unique()\n",
    "\n",
    "    df_priority_temp = df_select_pool_restrict_temp[df_select_pool_restrict_temp['ticker'].isin(last_portfolio_ticker_list)].copy()\n",
    "    df_priority_temp.sort_values(['predict','dividend_1Y_sum_yield'], ascending=False, inplace=True)\n",
    "    df_priority_temp.reset_index(drop=True, inplace=True)\n",
    "    df_priority = df_priority_temp.head(remain_number)\n",
    "\n",
    "\n",
    "    df_added = df_new_temp[df_new_temp['ticker'].isin(df_priority['ticker'].unique()) == False].head(base_portfolio_number - len(df_priority)).reset_index(drop=True)\n",
    "    df_triviality = df_select_pool_restrict_temp[df_select_pool_restrict_temp['ticker'].isin(df_added['ticker'].unique())].reset_index(drop=True)\n",
    "\n",
    "    df_restrict_result_temp = pd.concat([df_priority, df_triviality], axis=0)\n",
    "\n",
    "    # concat the df_restrict_result_temp to df_restrict_result\n",
    "    df_restrict_result = pd.concat([df_restrict_result, df_restrict_result_temp], axis=0)\n",
    "\n",
    "df_restrict_result.reset_index(drop=True, inplace=True)\n",
    "df_restrict_result_return = df_restrict_result.groupby('date')['y'].mean()\n",
    "\n",
    "print(f'./data/model/{model_folder_name}/df_restrict_result_{apply_restrict_index}_{base_portfolio_number}_{restrict_portfolio_number}')\n",
    "df_restrict_result.to_feather(f'./data/model/{model_folder_name}/df_restrict_result_{apply_restrict_index}_{base_portfolio_number}_{restrict_portfolio_number}.feather')\n",
    "print(f\"SR: {((df_restrict_result_return + 1).prod()**(4/len(df_restrict_result_return)) -1 ) / (df_restrict_result_return.std() * np.sqrt(4)):.2f}\")\n",
    "# Assuming your DataFrame is named df_returns\n",
    "cagr = calculate_cagr(df_restrict_result_return)\n",
    "print(f\"CAGR: {cagr:.2%}\")\n",
    "print(f\"Std: {(df_restrict_result_return.std() * np.sqrt(4)):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consider Turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_buffer_index = 0\n",
    "base_portfolio_number = 50\n",
    "buffer_portfolio_number = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自 2005-03-15 開始使用 buffer 建構投組\n",
      "df_buffer_result['date'].max(): NaT\n",
      "SR: 0.65\n",
      "CAGR: 14.37%\n",
      "Std: 22.12%\n"
     ]
    }
   ],
   "source": [
    "df_select_pool = df_select_pool.reset_index(drop=True)\n",
    "# get the base portfolio by base_portfolio_number \n",
    "df_select_pool_base = df_select_pool.groupby('date').apply(top_n, n = base_portfolio_number, columns=['predict_rank'], ascending=[True]).reset_index(drop=True)\n",
    "# get the buffer portfolio by buffer_portfolio_number\n",
    "df_select_pool_buffer = df_select_pool.groupby('date').apply(top_n, n = buffer_portfolio_number, columns=['predict_rank'], ascending=[True]).reset_index(drop=True)\n",
    "df_select_pool_buffer.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "apply_buffer_date = np.sort(reb_lst)[apply_buffer_index]\n",
    "# string format\n",
    "apply_buffer_date = pd.to_datetime(apply_buffer_date).strftime('%Y-%m-%d')\n",
    "print(f'自 {apply_buffer_date} 開始使用 buffer 建構投組')\n",
    "\n",
    "df_buffer_result = pd.DataFrame([])\n",
    "# concat the data that date is smaller than apply_buffer_date to df_buffer_result\n",
    "df_buffer_result = pd.concat([df_buffer_result, df_select_pool_base[df_select_pool_base['date'] < apply_buffer_date]], axis=0)\n",
    "# print the max date in df_buffer_result\n",
    "print(f\"df_buffer_result['date'].max(): {df_buffer_result['date'].max()}\")\n",
    "\n",
    "for i in range(apply_buffer_index, len(reb_lst)):\n",
    "    # get the slice data by reb_lst[i] in df_select_pool_buffer\n",
    "    df_select_pool_buffer_temp = df_select_pool_buffer[df_select_pool_buffer['rebalance_date'] == reb_lst[i]].reset_index(drop=True)\n",
    "    # get the slice data by reb_lst[i-1] in df_buffer_result, [i-1] is because we need to get the last portfolio\n",
    "    df_buffer_result_temp = df_buffer_result[df_buffer_result['rebalance_date'] == reb_lst[i - 1]].reset_index(drop=True)\n",
    "\n",
    "    # prepare the dataframe for finding the joint ticker\n",
    "    df_new_temp = df_select_pool_buffer_temp[['date','ticker']].copy()\n",
    "    df_new_temp['buffer'] = 1\n",
    "    df_old_temp = df_buffer_result_temp[['date','ticker']].copy()\n",
    "    df_old_temp['last_portfolio'] = 1\n",
    "\n",
    "    # merge the df_old_temp and df_new_temp\n",
    "    df_joint_temp = pd.merge(df_old_temp[['ticker','last_portfolio']], df_new_temp[['ticker','buffer']], on=['ticker'], how='outer')\n",
    "    df_joint_temp['buffer'].fillna(0, inplace=True)\n",
    "    df_joint_temp['last_portfolio'].fillna(0, inplace=True)\n",
    "    df_joint_temp['total'] = df_joint_temp['buffer'] + df_joint_temp['last_portfolio']\n",
    "    df_joint_temp = df_joint_temp[df_joint_temp['total'] == 2]\n",
    "\n",
    "    # get the joint ticker\n",
    "    joint_ticker = df_joint_temp['ticker'].unique()\n",
    "\n",
    "    # get the priority portfolio by joint_ticker\n",
    "    df_priority = df_select_pool_buffer_temp[df_select_pool_buffer_temp['ticker'].isin(joint_ticker)]\n",
    "    # get the triviality portfolio by EXCLUDING joint_ticker\n",
    "    df_triviality = df_select_pool_buffer_temp[df_select_pool_buffer_temp['ticker'].isin(joint_ticker) == False]\n",
    "    # then find the (top n - len(df_priority)) number of stocks triviality portfolio\n",
    "    df_triviality = df_triviality.groupby('date').apply(top_n, n = base_portfolio_number - len(df_priority), columns=['predict_rank'], ascending=[True]).reset_index(drop=True)\n",
    "    \n",
    "    # concat the df_priority and df_triviality\n",
    "    df_buffer_result_temp = pd.concat([df_priority, df_triviality], axis=0)\n",
    "\n",
    "    # concat the df_buffer_result_temp to df_buffer_result\n",
    "    df_buffer_result = pd.concat([df_buffer_result, df_buffer_result_temp], axis=0)\n",
    "\n",
    "df_buffer_result.reset_index(drop=True, inplace=True)\n",
    "df_buffer_result_return = df_buffer_result.groupby('date')['y'].mean()\n",
    "\n",
    "df_buffer_result.to_feather(f'./data/model/{model_folder_name}/df_buffer_result_{apply_buffer_index}_{base_portfolio_number}_{buffer_portfolio_number}.feather')\n",
    "print(f\"SR: {((df_buffer_result_return + 1).prod()**(4/len(df_buffer_result_return)) -1 ) / (df_buffer_result_return.std() * np.sqrt(4)):.2f}\")\n",
    "# Assuming your DataFrame is named df_returns\n",
    "cagr = calculate_cagr(df_buffer_result_return)\n",
    "print(f\"CAGR: {cagr:.2%}\")\n",
    "print(f\"Std: {(df_buffer_result_return.std() * np.sqrt(4)):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
